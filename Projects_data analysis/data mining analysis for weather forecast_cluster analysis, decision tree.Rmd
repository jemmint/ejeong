---
title: "HW2: Cluster Analysis and Decision Tree Induction"
author: "Eunmi(Ellie) Jeong"
date: '2020 10 10 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA,  warning = FALSE, message = FALSE)
```

## Section 1. Data preparation

```{r echo=FALSE}
setwd("~/Desktop/SU IM DOC/'20 FALL/IST707/assignment/HW2")
library(tidyverse)
library(arules)
library(imputeTS)
library(ggplot2)
library(caret)
library(e1071)
library(corrplot)
library(rpart)
library(writexl)
library(rattle)
library(pROC)
library(dplyr)
library(scales)
```
1. Check the structure of dataset (training dataset)
```{r results='hide'}
data <- read.csv("Weather Forecast Training.csv")
summary(data)
str(data) #51978 obs. 16 vars
```
2. Data cleaning to remove data quality issues (missing data, duplicate data, biased data, outliers/noises, attributes with no or low variance)
 * Missing data
    + Delete attributes which include too many missing values (more than 5% of total number of instances  51987*0.05 = 2599) 
    + Deleted attributes: Evaporation, Sunshine, WindGustSpeed, WindGustDir Pressure, Cloud
    + na_interpolation: if the number of NAs is less than 2599
    + Remove blanks
```{r results='hide'}
sum(is.na(data))
data2 <- subset(data,select=-c(Evaporation, Sunshine, WindGustSpeed, WindGustDir, Pressure, Cloud ))
data2$MinTemp <- na_interpolation(data2$MinTemp)
data2$MaxTemp <- na_interpolation(data2$MaxTemp)
data2$Rainfall <- na_interpolation(data2$Rainfall)
data2$WindSpeed <- na_interpolation(data2$WindSpeed)
data2$Humidity <- na_interpolation(data2$Humidity)
data2$Temp <- na_interpolation(data2$Temp)
levels(data2$RainToday) #""=747
data2 <- data2[!(data2$RainToday==""),]
data2$RainToday <- as.character(data2$RainToday)
data2$RainToday <- as.factor(data2$RainToday)
sum(!complete.cases(data2)) #no missing data
```
 * Check duplicate data (none)
```{r}
nrow(data2[!duplicated(data2), ]) #51231 (no duplicates)
```
 * Remove low or no variance (none)
```{r}
caret::nearZeroVar(data2, saveMetrics = T)
```
 * Identify&remove noises & outliers
```{r}
boxplot(data2$MinTemp)
data2$MinTemp[data2$MinTemp %in% boxplot.stats(data2$MinTemp)$out] <- median(data2$MinTemp, na.rm = T)
boxplot(data2$MaxTemp)
data2$MaxTemp[data2$MaxTemp %in% boxplot.stats(data2$MaxTemp)$out] <- median(data2$MaxTemp, na.rm = T)
boxplot(data2$Rainfall)
data2$Rainfall[data2$Rainfall %in% boxplot.stats(data2$Rainfall)$out] <- median(data2$MinTemp, na.rm = T)
boxplot(data2$WindSpeed)
data2$WindSpeed[data2$WindSpeed %in% boxplot.stats(data2$WindSpeed)$out] <- median(data2$WindSpeed, na.rm = T)
boxplot(data2$Humidity)
boxplot(data2$Temp)
data2$Temp[data2$Temp %in% boxplot.stats(data2$Temp)$out] <- median(data2$Temp, na.rm = T)
```
3. Discretization (for 'Decision Tree' modeling which work with both categorical and numerical attributes)
```{r}
data2$MinTemp_grp <- ifelse(data2$MinTemp > 0,"aboveZero", "belowZero")
data2 %>%
  group_by(MinTemp_grp) %>%
  summarise(avg = mean(MinTemp), count = n(), min = min(MinTemp), max = max(MinTemp))
  data2$MaxTemp_grp <- ifelse(data2$MaxTemp >= 30,"high", ifelse(data2$MaxTemp >= 15, "middle", "low"))
data2 %>%
  group_by(MaxTemp_grp) %>%
  summarise(avg = mean(MaxTemp), count = n(), min = min(MaxTemp), max = max(MaxTemp))
  data2$Rainfall_grp <- ifelse(data2$Rainfall > 0,"Yes", "No")
data2$WS_grp <- ifelse(data2$WindSpeed >= 25,"high", ifelse(data2$WindSpeed >= 15, "middle", "low"))
data2 %>%
  group_by(WS_grp) %>%
  summarise(avg = mean(WindSpeed), count = n(), min = min(WindSpeed), max = max(WindSpeed))
  data2$H_grp <- ifelse(data2$Humidity >= 70,"high", ifelse(data2$Humidity < 40, "low", "middle"))
data2 %>%
  group_by(H_grp) %>%
  summarise(avg = mean(Humidity), count = n(), min = min(Humidity), max = max(Humidity))
  data2$Temp_grp <- ifelse(data2$Temp >= 30,"high", ifelse(data2$Temp >= 15, "middle", "low"))
data2 %>%
  group_by(Temp_grp) %>%
  summarise(avg = mean(Temp), count = n(), min = min(Temp), max = max(Temp))
```
```{r results='hide'}
data2$MinTemp_grp <- as.factor(data2$MinTemp_grp)
data2$MaxTemp_grp <- as.factor(data2$MaxTemp_grp)
data2$Rainfall_grp <- as.factor(data2$Rainfall_grp)
data2$WS_grp <- as.factor(data2$WS_grp)
data2$H_grp <- as.factor(data2$H_grp)
data2$Temp_grp <- as.factor(data2$Temp_grp)
```
4. Scaling
```{r results='hide'}
data3 <- scale(data2[complete.cases(data2), sapply(data2, is.numeric)], center = T, scale = T)
```
5. Check the structure of dataset (testing dataset)
```{r results='hide'}
test <- read.csv("Weather Forecast Testing.csv")
str(test) #12994 obs, 16 vars
```
6. Data preprocessing (cleaning, na interpolation, discretization, scaling)
```{r results='hide'}
test2 <- subset(test,select=-c(Evaporation, Sunshine, WindGustSpeed, WindGustDir, Pressure, Cloud ))
test2$MinTemp <- na_interpolation(test2$MinTemp)
test2$MaxTemp <- na_interpolation(test2$MaxTemp)
test2$Rainfall <- na_interpolation(test2$Rainfall)
test2$WindSpeed <- na_interpolation(test2$WindSpeed)
test2$Humidity <- na_interpolation(test2$Humidity)
test2$Temp <- na_interpolation(test2$Temp)
levels(test2$RainToday) #""=161
test2 <- test2[!(test2$RainToday==""),]
test2$RainToday <- as.character(test2$RainToday)
test2$RainToday <- as.factor(test2$RainToday)
summary(test2$RainToday)
sum(!complete.cases(test2))
test2$ID <- as.character(test2$ID)

test2_dt <- subset(test2,select=c(1:10))
test2_dt$MinTemp_grp <- ifelse(test2_dt$MinTemp > 0,"aboveZero", "belowZero")
test2_dt$MaxTemp_grp <- ifelse(test2_dt$MaxTemp >= 30,"high", ifelse(test2_dt$MaxTemp >= 15, "middle", "low"))
test2_dt$Rainfall_grp <- ifelse(test2_dt$Rainfall > 0,"Yes", "No")
test2_dt$WS_grp <- ifelse(test2_dt$WindSpeed >= 25,"high", ifelse(test2_dt$WindSpeed >= 15, "middle", "low"))
test2_dt$H_grp <- ifelse(test2_dt$Humidity >= 70,"high", ifelse(test2_dt$Humidity < 40, "low", "middle"))
test2_dt$Temp_grp <- ifelse(test2_dt$Temp >= 30,"high", ifelse(test2_dt$Temp >= 15, "middle", "low"))

test2_dt$MinTemp_grp <- as.factor(test2_dt$MinTemp_grp)
test2_dt$MaxTemp_grp <- as.factor(test2_dt$MaxTemp_grp)
test2_dt$Rainfall_grp <- as.factor(test2_dt$Rainfall_grp)
test2_dt$WS_grp <- as.factor(test2_dt$WS_grp)
test2_dt$H_grp <- as.factor(test2_dt$H_grp)
test2_dt$Temp_grp <- as.factor(test2_dt$Temp_grp)
test3 <- scale(test2[complete.cases(test2), sapply(test2, is.numeric)], center = T, scale = T)
```

7. Exploratory Data Analysis (EDA) for training data
 * Pie-chart: Rainfall group
 
```{r}
data2 %>%  group_by(Rainfall_grp) %>%
  summarise(pct = percent(n()/nrow(data2))) %>%
  ggplot(aes(x = factor(1), y = pct, fill = Rainfall_grp)) +
  geom_bar(stat = "identity") +
  coord_polar(theta = "y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())
```
 
* Pie-chart: Minimum temperature group
  
```{r}
data2 %>%  group_by(MinTemp_grp) %>%
  summarise(pct = percent(n()/nrow(data2))) %>%
  ggplot(aes(x = factor(1), y = pct, fill = MinTemp_grp)) +
  geom_bar(stat = "identity") +
  coord_polar(theta = "y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())
```
 
* Histogram: Humidity, rainfall, temperature
 
```{r}
hist(data2$Humidity)
hist(data2$Rainfall)
hist(data2$Temp)
```
 
* Heatmap: Rainfall group-Humidity group-Temperature

```{r}
ggplot(data2, aes(x = H_grp, y = Rainfall_grp)) +
  geom_tile(aes(fill = Temp), color = "white") +
  scale_fill_gradient(low = "red", high = "green") +
  theme(axis.ticks = element_blank()) +
  labs(x = "Humidity", y = "Rainfall")
```

* Attribute's Correlation
    + Positive relationship: MinTemp-MaxTemp, Min/MaxTemp-Temp, Rainfall-Humidity/MinTemp, WindSpeed-Rainfall/MinTemp, 
    + Negative relationship: Rainfall-MaxTemp/Temp, Humidity-MaxTemp
    
```{r}
cor_matrix <- cor(data2[complete.cases(data2), sapply(data2, is.numeric)], method = "pearson")
corrplot(cor_matrix, type="upper", tl.cex = 0.7 ) 
```

## Section 2: Build, tune and evaluate cluster analysis and decision tree models
## Section 3. Prediction and interpretation 

1. Clustering analysis - kmeans 
 * Elbow method to decide optimal number of clusters: optimized number of clusters -> 3
 
```{r echo=FALSE}
set.seed(8)
wss <- function(k){
  return(kmeans(data3, k, nstart = 25)$tot.withinss)
}

k_values <- 1:15

wss_values <- purrr::map_dbl(k_values, wss)

plot(x = k_values, y = wss_values, 
     type = "b", frame = F,
     xlab = "Number of clusters K",
     ylab = "Total within-clusters sum of square")
```
 
* kmeans function

```{r}
km_output <- kmeans(data3, centers = 2, nstart = 25, iter.max = 100, algorithm = "Hartigan-Wong")
str(km_output)
```

* Hyperparameters' tuning 
    + centers: number of clusters 
    + iter.max: number of repeats
  
* Performance evaluation metrics
    + withinss: within cluster sum of squares (the lower, the better)
    + betweenss: between clusters sum of squares (the higher, the better)
    
```{r}
km1 <- kmeans(data3, centers = 2, nstart = 25, iter.max = 1000, algorithm = "Hartigan-Wong") #no change
str(km1) 
```
    
* km2: increase centers, iter.max -> lower withinss(more condensed), higher betweenss(different clusters are more separated)
    
```{r}
km2 <- kmeans(data3, centers = 3, nstart = 25, iter.max = 1000, algorithm = "Hartigan-Wong") 
str(km2)
```

* kmeans with PCA : importance of components:PC1, PC2 (low variance)

```{r}
pca <- prcomp(data3, scale. = T, center = T)
print(pca)
summary(pca)
pcs <- as.data.frame(predict(pca, newdata = data3))
cluster <- km2$cluster
pcs$cluster <- cluster[match(rownames(pcs), names(cluster))]
qplot(PC1, PC2, colour = cluster, data = pcs)
```

* Repurpose clustering analysis for classification - kmeans
    + The purpose of clustering analysis is grouping unlabeled data. Therefore it does not pay attention to the prediction functionality.
    + However, we can classify sub-datasets of clusters that we defined through clustering analysis.
    + From trainig model, I induced that cluster 1 is related to raining, and cluster 2 is related to not raining. 
    
```{r}
pcs1 <- as.data.frame(predict(pca, newdata = data3))
cluster1 <- km1$cluster
pcs1$cluster1 <- cluster1[match(rownames(pcs1), names(cluster1))]
qplot(PC1, PC2, colour = cluster1, data = pcs1)
training <- data2[complete.cases(data2), sapply(data2, is.numeric)]
training$cluster <- cluster1
training$RainTomorrow <- data2$RainTomorrow
table(cluster=training$cluster, RainTomorrow=training$RainTomorrow)
training$Rain <- ifelse(training$RainTomorrow=="Yes","1", "2")
training$Rain <- as.numeric(training$Rain) 
```

*Based on this assumption, I figured out values for 'RainTomorrow' attribute of training dataset. (km1 model was used since it's a binary question)

```{r}
km_test <- kmeans(test3, centers = 2, nstart = 25, iter.max = 1000, algorithm = "Hartigan-Wong")
```

```{r echo=FALSE}
cluster_t <- km_test$cluster
test2$cluster <- cluster_t
test2$RainTomorrow <- test2$cluster
test2$RainTomorrow <- as.factor(test2$RainTomorrow)
test2$RainTomorrow <- ifelse(test2$RainTomorrow=="1", "Yes", "No")
write_xlsx(test2,"C:\\Users\\jemmint\\Desktop\\SU IM DOC\\kmean.xlsx")
```
2. Clustering analysis - HAC 
 * I used random sampling due to repeated errors caused by big size of dataset )
```{r results='hide'}
sample_train <- data2 %>%
  sample_frac(size = 0.005, replace = FALSE)
```
 * Modeling
```{r}
hac_output <- hclust(dist(sample_train[complete.cases(sample_train), sapply(sample_train, is.numeric)], method = "euclidean"), method = "complete")
plot(hac_output) 
```

* Hyperparameters' tuning 
    + method(intracluster): distance function between data points 
    + method(intercluster): definition of intercluster distance
  
* Performance evaluation metrics 
    + single linkage: useful for clusters of different size
    + complete linkage: useful for clusters of similar size
    + average, centroid linkage: resistant to outliers
    
```{r}
hac1 <- hclust(dist(sample_train[complete.cases(sample_train), sapply(sample_train, is.numeric)], method = "euclidean"), method = "single")
plot(hac1) 
hac2 <- hclust(dist(sample_train[complete.cases(sample_train), sapply(sample_train, is.numeric)], method = "euclidean"), method = "average")
plot(hac2) 
hac3 <- hclust(dist(sample_train[complete.cases(sample_train), sapply(sample_train, is.numeric)], method = "euclidean"), method = "centroid")
plot(hac3) 
hac4 <- hclust(dist(sample_train[complete.cases(sample_train), sapply(sample_train, is.numeric)], method = "manhattan"), method = "centroid")
plot(hac4) 
```

* Cutting 2 clusters (k=2)
   + Share the same features with kmeans regarding prediction 
   + Apply same technique for testing dataset classification 
   + From training model, I induced that cluster 1 is related to raining, and cluster 2 is related to not raining.
   
```{r}
```

* Identify subset of the clusters
    + cluster 1: Yes(rain)
    + cluster 2: No(no rain)

```{r echo=FALSE, results='hide'}
tree <- cutree(hac_output, k=2)
t <- list(tree)
t <- as.data.frame(t)
table(t)
colnames(t)
t <- rename(t, "cluster"="c..1....1L...2....2L...3....2L...4....2L...5....1L...6....2L..")
sample_train$cluster <- t$cluster
table(cluster=sample_train$cluster, RainTomorrow=sample_train$RainTomorrow)
```

* Repurpose clustering analysis for classification - HAC

```{r}
hac_test <- hclust(dist(test3, method = "euclidean"), method = "centroid")
plot(hac_test)
tree2 <- cutree(hac_test, k=2)
```

```{r echo=FALSE, results='hide'}
t2 <- list(tree2)
t2 <- as.data.frame(t2)
table(t2)
```

```{r echo=FALSE}
colnames(t2)
t2 <- rename(t2, "cluster"="c..1....1L...2....1L...3....1L...4....1L...5....1L...6....1L..")
test2_hac <- subset(test2,select=c(1:10))
test2_hac$cluster <- t2$cluster
test2_hac$RainTomorrow <- test2_hac$cluster
test2_hac$RainTomorrow <- as.factor(test2_hac$RainTomorrow)
test2_hac$RainTomorrow <- ifelse(test2_hac$RainTomorrow=="1", "Yes", "No")
write_xlsx(test2_hac,"C:\\Users\\jemmint\\Desktop\\SU IM DOC\\hac.xlsx")
```

3. Decision Tree
 * Modeling
     + Modeling with numerical + categorical attributes 
     
```{r}
dt_model <- train(RainTomorrow ~ ., data = data2, metric = "Accuracy", method = "rpart")
print(dt_model)
print(dt_model$finalModel)
```
    
* Modeling with categorical attributes only 
    
```{r}
category_train <- data2[complete.cases(data2), sapply(data2, is.factor)]
dt_model2 <- train(RainTomorrow ~ ., data = category_train, metric = "Accuracy", method = "rpart")
print(dt_model2)
```

* Hyperparameters' tuning 
    + minsplit: the minimum number of observations that must exist in a node 
    + minbucket: the minimum number of observations in any terminal node  
    + maxdepth: maximum depth of any any node of the final tree
    + cp: model complexity 
    + To make unbiased and low-variance model: minsplit, minbucket,cp should be high, and maxdepth should be low
    + However, bias and complexity have tradeoffs. For example, as cp(complexity) gets higher, accuracy and kappa(bias) gets lower. Therefore, we need to find the balanced model.
  
* Performance evaluation method
    + Confusion matrix, Cross-validation, ROC curve, Bootstrapping
  
* Performance evaluation metrics 
    + accuracy, kappa, precision, recall, F-measure, AUC(area under curve) : high in good models

  
```{r}
dt_model3 <- train(RainTomorrow ~ ., data = data2, method = "rpart",
                   metric = "Accuracy",
                   tuneLength = 8,
                   control = rpart.control(minsplit = 10, minbucket = 10, maxdepth = 3))
dt_model3

tr_control <- trainControl(method = "cv", number = 3)
dt_model_cv <- train(RainTomorrow ~ .,
                     data = data2, method = "rpart", metric = "Accuracy",
                     control = rpart.control(minsplit = 50, minbucket = 20,
                                             maxdepth = 5, cp = 0.01),
                     trControl = tr_control, na.action = na.omit)
print(dt_model_cv)
```

* Decision Tree model visualization

```{r}
dt_model_roc <- train(RainTomorrow ~ ., data = data2, method = "rpart", metric = "ROC",
                     trControl = trainControl(method = "cv", number = 5, classProbs = T,
                                              summaryFunction = twoClassSummary),
                     tuneGrid = expand.grid(cp = seq(0, 0.01, 0.001)),
                     control = rpart.control(minsplit = 3, minbucket = 1))
print(dt_model_roc)
fancyRpartPlot(dt_model_roc$finalModel)

```

* Decision Tree model interpretation
    + Humidity, rainfall, and wind speed are major features related to predict whether it will rain or not tomorrow.
    + It is more likely to rain when humidity is higher than 65%.
    + Wind speed higher than 17km/hr, and the amount of rainfall for the day over 2.9mm contribute to rain for tomorrow.
    + If the amount of rainfall for the day is less than 0.35mm, humidity is lower than 52mm, and minimum temperature is lower than 24C, it is less likely to rain tomorrow. 

* Prediction on testing data

```{r results='hide'}
dt_predict <- predict(dt_model_roc, newdata = test2_dt, type = "raw")
```

```{r echo=FALSE}
dt_t <- list(dt_predict)
dt_t <- as.data.frame(dt_t)
colnames(dt_t)
dt_t <- rename(dt_t, "RainTomorrow"= "structure.c.2L..1L..1L..1L..1L..2L..1L..2L..1L..1L..2L..1L..2L..")
test2_dt$RainTomorrow <- dt_t$RainTomorrow
write_xlsx(test2_dt,"C:\\Users\\jemmint\\Desktop\\SU IM DOC\\dt.xlsx")
```

* Performance evaluation
    + ROC curve and AUC (ability of a classifier to distinguish between classes-Yes/No)
    + AUC(training):0.805
    + AUC(testing) 1 (As the training dataset does not have actual value to compare, the predicted values themselves are used for validation)

```{r}
dt_plot <- train(RainTomorrow ~ ., data = data2, method = "rpart", tuneLength = 10)
dt_pred_prob0 <- predict(dt_plot, newdata = data2, type = "prob")
head(dt_pred_prob0, n = 5)
roc_curve0 <- roc(data2$RainTomorrow,dt_pred_prob0$Yes)
plot(roc_curve0)
auc(roc_curve0)
dt_pred_prob <- predict(dt_model_roc, newdata = test2_dt, type = "prob")
head(dt_pred_prob, n = 5)
roc_curve <- roc(test2_dt$RainTomorrow,dt_pred_prob$Yes)
plot(roc_curve)
auc(roc_curve) 
```

* Confusion matrix

```{r}
prop.table(table(dt_predict, test2_dt$RainTomorrow), margin = NULL)
confusionMatrix(dt_predict, test2_dt$RainTomorrow)
```

* Precision, recall, F-measure

```{r}
precision <- posPredValue(dt_predict, test2_dt$RainTomorrow, positive = "yes")
recall <- sensitivity(dt_predict, test2_dt$RainTomorrow, positive = "yes")
f <- 2 * precision * recall / (precision + recall)
sprintf("Precision is %.2f; recall is %.2f; F measure if %.2f",
        precision, recall, f)
```

4. Best models & parameters

 Type  |  Model ID   | Model specifications
-------|-------------|----------------------------------------------------------------
kmeans |   km2       | centers=3, nstart=25, iter.max=1000, low withinss(condensed), high betweenss(clusters are separated)
HAC    |   hac1      | method(intracluster)=euclidean, method(intercluster)=single, useful for clusters of different size 
DT     | dt_model_cv | minsplit=50, minbucket=10, maxdepth=5, cp=0.01. highest accuracy and kappa





---
title: 'HW3: NBC, KNN, SVM, and Ensemble Learning'
author: "Eunmi(Ellie) Jeong"
date: '2020 11 01 '
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA,  warning = FALSE, message = FALSE)
```

## Section 1. Data preparation

```{r echo=FALSE}
setwd("~/Desktop/SU IM DOC/'20 FALL/IST707/assignment/HW3")
library(mlbench)
library(tidyverse)
library(arules)
library(imputeTS)
library(ggplot2)
library(caret)
library(e1071)
library(corrplot)
library(writexl)
library(pROC)
library(klaR)
library(dplyr)
library(scales)
library(kernlab)
library(randomForest)
library(xgboost)
library(gbm)
```

### 1-1. Data preparation-training data
 Data cleaning to remove data quality issues (missing data, duplicate data, biased data, outliers/noises, attributes with no or low variance)

```{r results='hide'}
data <- read.csv("Disease Prediction Training.csv")
str(data) #49000 obs. 12 vars
summary(data)
```

* Check missing data

```{r}
sum(is.na(data)) #no missing data
```

* Check and remove duplicate data record

```{r results='hide'}
nrow(data[duplicated(data), ]) #1752 duplicates
data[duplicated(data)|duplicated(data, fromLast = T), ]
data <- data %>%
  distinct(.keep_all = T) 
```

* Remove low or no variance 

```{r}
caret::nearZeroVar(data, saveMetrics = T) #none 
```

* Identify&remove noises & outlier

```{r}
boxplot(data$Age)
data$Age[data$Age %in% boxplot.stats(data$Age)$out] <- median(data$Age, na.rm = T)
boxplot(data$Height)
data$Height[data$Height %in% boxplot.stats(data$Height)$out] <- median(data$Height, na.rm = T)
boxplot(data$Weight) #twice
data$Weight[data$Weight %in% boxplot.stats(data$Weight)$out] <- median(data$Weight, na.rm = T)
data$Weight[data$Weight %in% boxplot.stats(data$Weight)$out] <- median(data$Weight, na.rm = T)
boxplot(data$High.Blood.Pressure)
data$High.Blood.Pressure[data$High.Blood.Pressure %in% boxplot.stats(data$High.Blood.Pressure)$out] <- median(data$High.Blood.Pressure, na.rm = T)
boxplot(data$Low.Blood.Pressure)
data$Low.Blood.Pressure[data$Low.Blood.Pressure %in% boxplot.stats(data$Low.Blood.Pressure)$out] <- median(data$Low.Blood.Pressure, na.rm = T)
```

* Change attribute type

```{r results='hide'}
data$Smoke <- as.factor(data$Smoke)
data$Alcohol <- as.factor(data$Alcohol)
data$Exercise <- as.factor(data$Exercise)
data$Disease <- as.factor(data$Disease)
```

* Discretization: necessary for Naive Bayes Classifier

```{r}
data$age_grp <- arules::discretize(data$Age, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data %>%
  group_by(age_grp) %>%
    summarise(avg = mean(Age), count = n(), min = min(Age), max = max(Age))

data$height_grp <- arules::discretize(data$Height, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data %>%
  group_by(height_grp) %>%
  summarise(avg = mean(Height), count = n(), min = min(Height), max = max(Height))

data$weight_grp <- arules::discretize(data$Weight, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data %>%
  group_by(weight_grp) %>%
  summarise(avg = mean(Weight), count = n(), min = min(Weight), max = max(Weight))

data$h_blood_grp <- arules::discretize(data$High.Blood.Pressure, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data %>%
  group_by(h_blood_grp) %>%
  summarise(avg = mean(High.Blood.Pressure), count = n(), min = min(High.Blood.Pressure), max = max(High.Blood.Pressure))

data$l_blood_grp <- arules::discretize(data$Low.Blood.Pressure, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data %>%
  group_by(l_blood_grp) %>%
  summarise(avg = mean(Low.Blood.Pressure), count = n(), min = min(Low.Blood.Pressure), max = max(Low.Blood.Pressure))
training_cat <- data[sapply(data, is.factor)]
```

* Numerization: necessary for KNN, RF(Random Forest), GBM(Gradient Boosting Machine), SVM(linear & non-linear)
* Standardization & Normalization: necessary for numerical values which will be used in KNN, RF(Random Forest), GBM(Gradient Boosting Machine), SVM(linear & non-linear)
 
```{r}
dmy <- caret::dummyVars(~Gender+Cholesterol+Glucose+Smoke+Alcohol+Exercise,data = data, fullRank = T)
dummy <- data.frame(predict(dmy, newdata = data))
subset <- subset(data,select= c(Age, Height, Weight, High.Blood.Pressure, Low.Blood.Pressure, Disease))
training_num <- cbind(subset, dummy) 
pre_process <- preProcess(training_num, method = c("scale", "center"))
training_num_s <- predict(pre_process, newdata = training_num)
```

* Check attribute's correlation
    + Positive relationship: Glucose-Cholesterol, Cholesterol-high/low blood presssure, male-smoke, male-alcohol, male-height, alcohol-smoke, height-weight, hight blood pressure-low blood pressure
    + Negative relationship: age-Cholesterol, age-height

```{r}
cor_matrix <- cor(training_num_s [complete.cases(training_num_s ), sapply(training_num_s , is.numeric)], method = "pearson")
corrplot(cor_matrix, type = "upper")

```

### 1-2. Data preparation-testing data
 Data cleaning to remove data quality issues (missing data, duplicate data, biased data, outliers/noises, attributes with no or low variance)
 
```{r results='hide'}
data2 <- read.csv("Disease Prediction Testing.csv")
str(data2) #21000 obs. 12 vars
```

* Check missing data

```{r}
sum(is.na(data2)) #no missing data
```

* Check and remove duplicate data record

```{r}
nrow(data2[duplicated(data2), ]) #no duplicates
```

* Remove low or no variance 

```{r}
caret::nearZeroVar(data2, saveMetrics = T) #none 
```

* Identify&remove noises & outlier

```{r}
boxplot(data2$Age)
data2$Age[data2$Age %in% boxplot.stats(data2$Age)$out] <- median(data2$Age, na.rm = T)
boxplot(data2$Height)
data2$Height[data2$Height %in% boxplot.stats(data2$Height)$out] <- median(data2$Height, na.rm = T)
boxplot(data2$Weight) #twice
data2$Weight[data2$Weight %in% boxplot.stats(data2$Weight)$out] <- median(data2$Weight, na.rm = T)
boxplot(data2$High.Blood.Pressure)
data2$Weight[data2$Weight %in% boxplot.stats(data2$Weight)$out] <- median(data2$Weight, na.rm = T)
boxplot(data2$High.Blood.Pressure)
data2$High.Blood.Pressure[data2$High.Blood.Pressure %in% boxplot.stats(data2$High.Blood.Pressure)$out] <- median(data2$High.Blood.Pressure, na.rm = T)
boxplot(data2$Low.Blood.Pressure)
data2$Low.Blood.Pressure[data2$Low.Blood.Pressure %in% boxplot.stats(data2$Low.Blood.Pressure)$out] <- median(data2$Low.Blood.Pressure, na.rm = T)
```

* Change attribute type

```{r results='hide'}
data2$ID <- as.factor(data2$ID)
data2$Smoke <- as.factor(data2$Smoke)
data2$Alcohol <- as.factor(data2$Alcohol)
data2$Exercise <- as.factor(data2$Exercise)
```

* Discretization: necessary for Naive Bayes Classifier

```{r}
data2$age_grp <- arules::discretize(data2$Age, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data2 %>%
  group_by(age_grp) %>%
  summarise(avg = mean(Age), count = n(), min = min(Age), max = max(Age))

data2$height_grp <- arules::discretize(data2$Height, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data2 %>%
  group_by(height_grp) %>%
  summarise(avg = mean(Height), count = n(), min = min(Height), max = max(Height))

data2$weight_grp <- arules::discretize(data2$Weight, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data2 %>%
  group_by(weight_grp) %>%
  summarise(avg = mean(Weight), count = n(), min = min(Weight), max = max(Weight))

data2$h_blood_grp <- arules::discretize(data2$High.Blood.Pressure, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data2 %>%
  group_by(h_blood_grp) %>%
  summarise(avg = mean(High.Blood.Pressure), count = n(), min = min(High.Blood.Pressure), max = max(High.Blood.Pressure))

data2$l_blood_grp <- arules::discretize(data2$Low.Blood.Pressure, method = "interval", breaks = 3, labels = c("low", "middle", "high"))
data2 %>%
  group_by(l_blood_grp) %>%
  summarise(avg = mean(Low.Blood.Pressure), count = n(), min = min(Low.Blood.Pressure), max = max(Low.Blood.Pressure))
testing_cat <- data2[sapply(data2, is.factor)]
```

* Numerization: necessary for KNN, RF(Random Forest), GBM(Gradient Boosting Machine), SVM(linear & non-linear)
* Standardization & Normalization: necessary for numerical values which will be used in KNN, RF(Random Forest), GBM(Gradient Boosting Machine), SVM(linear & non-linear)

```{r}
dmy2 <- caret::dummyVars(~Gender+Cholesterol+Glucose+Smoke+Alcohol+Exercise,data = data2, fullRank = T)
dummy2 <- data.frame(predict(dmy2, newdata = data2))
subset2 <- subset(data2,select= c(ID, Age, Height, Weight, High.Blood.Pressure, Low.Blood.Pressure))
testing_num <- cbind(subset2, dummy2) 
pre_process2 <- preProcess(testing_num, method = c("scale", "center"))
testing_num_s <- predict(pre_process2, newdata = testing_num)
```

### 1-3. Exploratory Data Analysis (EDA)-traning data
  
* Piechart: Gender distribution (Female>Male)

```{r}
data %>%  group_by(Gender) %>%
  summarise(pct = percent(n()/nrow(data))) %>%
  ggplot(aes(x = factor(1), y = pct, fill = Gender)) +
  geom_bar(stat = "identity") +
  coord_polar(theta = "y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())
```

* Piechart: Age_group distribution (Hight>Middle>Low)

```{r}
data %>%  group_by(age_grp) %>%
  summarise(pct = percent(n()/nrow(data))) %>%
  ggplot(aes(x = factor(1), y = pct, fill = age_grp)) +
  geom_bar(stat = "identity") +
  coord_polar(theta = "y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())
```

* Piechart: Disease distribution (Yes>No)

```{r}
data %>%  group_by(Disease) %>%
  summarise(pct = percent(n()/nrow(data))) %>%
  ggplot(aes(x = factor(1), y = pct, fill = Disease)) +
  geom_bar(stat = "identity") +
  coord_polar(theta = "y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())
```

* Boxplot: age group-High.Blood.Pressure / older people tend to have higher blood pressure

```{r}
ggplot(data, aes(x = age_grp, y = High.Blood.Pressure)) +
  geom_boxplot() +
  theme(panel.grid.major.x = element_blank())
```

* Boxplot: Glucose-Age / older people tend to have higher glucose

```{r}
ggplot(data, aes(x = Glucose, y = Age)) +
  geom_boxplot() +
  theme(panel.grid.major.x = element_blank())
```

* Density plot: Weight-Cholesterol / High Cholesterol level is concentrated on 70kg. Heavier people are more likely to have higher Cholesterol level

```{r}
ggplot(data, aes(Weight)) +
  geom_density(aes(fill=factor(Cholesterol))) +
  labs(title="Density plot",
       subtitle="Weight Grouped by Cholesterol level",
       caption="Source: data",
       x="Weight",
       fill="Cholesterol level")
```

* Heatmap: Alcohol-Smoke / People who drink and smoke show higher blood pressure, but smoke does not affect blood pressure by itself.

```{r}
ggplot(data, aes(x = Alcohol, y = Smoke)) +
  geom_tile(aes(fill = High.Blood.Pressure), color = "white") +
  scale_fill_gradient(low = "green", high = "red") +
  theme(axis.ticks = element_blank()) +
  labs(x = "Alcohol", y = "Smoke")
```

* Scatter plot: Weight-Age / Weight and Age are in positive relationship, but not very strong. The heavier, the older, people are more likely to get a disease.

```{r}
ggplot(data, aes(x = Weight, y = Age)) +
  geom_point(aes(color = Disease)) +
  geom_smooth(method = "lm")
```

### 1-4. Exploratory Data Analysis(EDA)-testing data (demographic analysis)
 
* Piechart: Gender distribution (Female>Male)

```{r}
data2 %>%  group_by(Gender) %>%
  summarise(pct = percent(n()/nrow(data2))) %>%
  ggplot(aes(x = factor(1), y = pct, fill = Gender)) +
  geom_bar(stat = "identity") +
  coord_polar(theta = "y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())
```

* Piechart: Age_group distribution (Hight>Middle>Low)

```{r}
data2 %>%  group_by(age_grp) %>%
  summarise(pct = percent(n()/nrow(data2))) %>%
  ggplot(aes(x = factor(1), y = pct, fill = age_grp)) +
  geom_bar(stat = "identity") +
  coord_polar(theta = "y") +
  theme(axis.ticks = element_blank(),
        axis.text = element_blank(),
        axis.title = element_blank())

```

* Density plot: Weight-Cholesterol / High Cholesterol level is concentrated on 70kg. Heavier people are more likely to have higher Cholesterol level

```{r}
ggplot(data2, aes(Weight)) +
  geom_density(aes(fill=factor(Cholesterol))) +
  labs(title="Density plot",
       subtitle="Weight Grouped by Cholesterol level",
       caption="Source: data2",
       x="Weight",
       fill="Cholesterol level")
```

**Test data shows similar demographic distribution(age, gender) and weight-Cholesterol density to those of training data**  

```{r}

```

## Section2. Build, tune and evaluate various machine learning algorithms 

### 2-1. Building machine learning algorithms

### 2-1-1. Naive Bayes Classifier(NBC)
* Building a model with default value
* Hyper-parameters: fL=0, usekernel=False

```{r}
nbc1 <- train(training_cat[,!names(training_cat)%in%"Disease"], training_cat$Disease, method = "nb")
nbc1
```

* Model tuning
    + fl: adding smoothing to fix zero probility issue (1~4)
    + kernel: probability density function for continuous attributes (True, False)
    
```{r}
nbc2 <- train(training_cat[,!names(training_cat)%in%"Disease"], training_cat$Disease, method = "nb",
                   trControl = trainControl(method = "cv", number = 3),
                   tuneGrid = expand.grid(fL = 1:4, usekernel = c(TRUE, FALSE), adjust = 1:3))
nbc2
nbc2$bestTune
```

* ROC & AUC 
    + nbc1: 0.7478

```{r}
roc_nbc1 <- predict(nbc1, newdata = training_cat, type = "prob")
roc_curve1 <- roc(training_cat$Disease,roc_nbc1$"1")
plot(roc_curve1)
auc(roc_curve1) 
```

    + nbc2: 0.7478 

```{r}
roc_nbc2 <- predict(nbc2, newdata = training_cat, type = "prob")
roc_curve2 <- roc(training_cat$Disease,roc_nbc2$"1")
plot(roc_curve2)
auc(roc_curve2) 
```

* Performance evalution method & metrics
    + Method: Cross validation, ROC
    + Metrics: accuracy, kappa (the higher, the better), AUC (the closer to 1, the more accurate/ no difference in nbc1 & nbc2)

* Best performing model & hyper-parameters
    + Model: nbc2
    + Hyper-parameters: fL=1, kernel=False
  
* Prediction 

```{r results='hide'}
nbc2_p <- predict(nbc2, newdata = testing_cat, type = "raw")
list_p <- subset(data2,select=ID)
list_p$NBC <- nbc2_p
```

### 2-1-2. KNN
* Buidling a model with default value
* Hyper-parameter: k=5,7,9, distance function='Euclidean' (default for numerical values) 
* I used random sampling due to long execution time

```{r}
set.seed(1)
sample_knn <- training_num_s %>%
  sample_frac(size = 0.2, replace = FALSE)
knn1 <- train(Disease ~ ., data = sample_knn, method = "knn")
print(knn1)
```

* Model tuning
    + k: number of neighbors (10~15)
   
```{r}
set.seed(1)
knn2 <- train(Disease ~ ., data = sample_knn, method = "knn",
                    tuneGrid = data.frame(k = seq(10, 15)),
                    trControl = trainControl(method = "repeatedcv",
                                             number = 5, repeats = 3))
print(knn2)
knn2$bestTune
```

* Performance evalution method & metrics
    + Method: Cross validation
    + Metrics: accuracy, kappa (the higher, the better)
    
*Best performing model & hyper-parameters
    + Model: knn2 
    + Hyper-parameters: k=15
  
* Prediction

```{r results='hide'}
knn2_p <- predict(knn2, newdata = testing_num_s)
list_p$KNN <- knn2_p
```

### 2-1-3. SVM (linear)
* Buidling & tuning a model (linear) 
* I used random sampling due to long execution time
* Hyper-parameter
    + C: Cost of constraints (0~2) 
    + Lower: soft margin, high bias, low variance -> underfitting 
    + Higher: hard margin, low bias, high variance -> overfitting
   
```{r}
set.seed(123)
sample <- training_num_s %>%
  sample_frac(size = 0.05, replace = FALSE)
svm_linear <- train(
  Disease ~., data = sample, method = "svmLinear",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(C = seq(0, 2, length = 20)),
)
svm_linear
svm_linear$bestTune
```

* Accuracy-Cost plot: 6 C values which show the highest accuracy (selected the smallest one among them)

```{r}
plot(svm_linear)
```

* Performance evalution method & metrics
    + Method: Cross validation
    + Metrics: accuracy, kappa (the higher, the better)

* Best performing model & hyper-parameters
    + Model: svm_linear
    + Hyper-parameters: C=0.2105263

* Prediction

```{r}
svm_linear_p <- predict(svm_linear, newdata = testing_num_s)
list_p$SVM_Linear <- svm_linear_p
```

### 2-1-4. SVM (non-linear)
* Buidling & tuning a model (non-linear) 
* I used random sampling due to long execution time
* Hyper-parameters
    + C: ost of constraints / Lower: soft margin, high bias, low variance -> underfitting / Higher: hard margin, low bias, high variance -> overfitting
    + Sigma: standard deviation of distribution / Lower: low bias, high variance -> overfitting / Higher: high bias, low variance -> underfitting

```{r}
svm_non_linear <- train(
  Disease ~., data = sample, method = "svmRadial",
  trControl = trainControl("cv", number = 10),
  tuneLength = 10
)
svm_non_linear 
svm_non_linear$bestTune
```

* Performance evalution method & metrics
    + Method: Cross validation
    + Metrics: accuracy, kappa (the higher, the better)
  
* Best performing model & hyper-parameters
    + Model: svm_non_linear
    + Hyper-parameters: C=0.5, Sigma=0.07043372

* Prediction

```{r}
svm_non_linear_p <- predict(svm_non_linear, newdata = testing_num_s)
list_p$SVM_RBF <- svm_non_linear_p
```

### 2-1-5. Random Forest
* Buidling & tuning a model
* I used random sampling due to long execution time
* Hyper-parameters: mtry (1~5)-the number of features to choose
    + Higher: trees look similar & higher probability of overfitting
    + Lower: trees look different & higher probability of underfitting
    
```{r}
fitControl <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
customGrid <- expand.grid(mtry = 1:5)
set.seed(1)
rf <- train(Disease ~ ., data = sample, method = "rf", trControl = fitControl, tuneGrid = customGrid)
rf
rf$bestTune
```

* Performance evalution method & metrics
    + Method: Cross validation
    + Metrics: accuracy, kappa (the higher, the better)
  
* Best performing model & hyper-parameters
    + Model: rf
    + Hyper-parameters: mtry=2

* Prediction

```{r}
rf_p <- predict(rf, newdata = testing_num_s)
list_p$RF <- rf_p
```

### 2-1-6. Gradient Boosting Machine (GBM)
* Buidling & tuning a model
* I used random sampling due to long execution time
* Hyper-parameters: 
    + nrounds: the number of boosting iteratoin 
    + max-depth: maximum depth of a tree. the higher, the more likely to overfit
    + eta: step size shrinkage used in update to prevents overfitting. 
    + gamma: minimum loss reduction required to make a further partition on a leaf node of the tree. the larger gamma is, the more conservative the algorithm will be.
    + colsample_bytree: subsample ratio of columns when constructing each tree. can be used to prevent overfitting.
    + min_child_weight: minimum sum of instance weight needed in a child. The larger, the more conservative the algorithm will be.
    + subsample: subsample ratio of the training instances. can be used to prevent overfitting.

```{r results='hide'}
set.seed(1)
gbm <- train(Disease ~ ., data = sample, 
                 method = "xgbTree")
gbm
gbm$bestTune
```

* Performance evalution method & metrics
    + Method: Cross validation
    + Metrics: accuracy, kappa (the higher, the better)
  
* Best performing model & hyper-parameters
    + Model: gbm
    + Hyper-parameters: nrounds=50, max_depth=1, eta=0.3, gamma=0, colsample_bytree=0.6, min_child_weight=1, subsample=1

* Prediction

```{r}
gbm_p <- predict(gbm, newdata = testing_num_s)
list_p$GBM <- gbm_p
```

### 2-2. Best performing models & model specification summary

Algorithm          | Model ID       | Specification
-------------------|----------------|------------------------
NBC                | nbc2           | fL=0, usekernel=False
KNN                | knn2           | k=15
SVM_Linear         | svm_linear     | C=0.2105263
SVM_non-linear     | svm_non_linear | Sigma=0.07043372, C=0.5
RF                 | rf             | mtry=2
GBM                | gbm            | nrounds=50, max_depth=1, eta=0.3, gamma=0, colsample_bytree=0.6, min_child_weight=1, subsample=1


## Section 3. Prediction and interpretation 
### 3-1. Exporting prediction in csv file

```{r}
write.csv(list_p,"C:\\Users\\jemmint\\Desktop\\SU IM DOC\\prediction.csv")
```

### 3-2. Conclusion

Overall, the performance of 6 algorithms (NBC, KNN, SVM_Linear, SVM_non_linear, RF, GBM) seems to be similar in terms of accuracy and kappa. All the best performing models show around 70% accuracy and 40& kappa. However, each algorithm has different features. NBC is good for handling categorical values which are independent each other, so it is robust to outliers. Also, it's helpful to get the probability of individual classes. However, additional technique(laplace smoothing ) is required to fix zero probabilities which can appear when new data is input. Except for NBC, 5 other algorithms can be used with numerical values. Among them, a distance-based algorithm, KNN, is sensitive to noises and outliers. Therefore, data cleaning and standardization should be done carefully at the data preprocessing stage. SVM is a discriminative black box model used for determining decision boundary. As it's a black-box model, interpretation is not easy. However, it can be utilized for various tasks since it can handle non-linear decision bounday. The ensemble algorithms RF and GBM are also useful for dealing with various tasks. RF is intuitive and robust to overfitting. GBM produces comparatively accurate prediction since it uses gradient descent to construct new base learners that minimize loss function. However, both RF and GBM require considerabe time for modeling and numerous hyper-parameters for tuning.    
 